"""
trainer.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ëª¨ë¸ í•™ìŠµ(train) ë° í‰ê°€(evaluate)ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
  - EarlyStopping, ReduceLROnPlateau, TensorBoard ì½œë°±
  - ì ì§„ì  í•™ìŠµ (gradual train): ì ì  ë°ì´í„°ë¥¼ ëŠ˜ë ¤ê°€ë©° í•™ìŠµ
  - í•™ìŠµ ì´ë ¥ CSV ì €ìž¥
"""

from __future__ import annotations

import os
from datetime import datetime
from pathlib import Path
from typing import Tuple, Dict, Optional

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ í•™ìŠµê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Trainer:
    """
    DeepMoney ëª¨ë¸ í•™ìŠµê¸°.

    Args:
        model:  ì»´íŒŒì¼ëœ keras.Model
        config: Config ë°ì´í„°í´ëž˜ìŠ¤
    """

    def __init__(self, model: keras.Model, config):
        self.model = model
        self.config = config
        self._history: Optional[keras.callbacks.History] = None

    # â”€â”€ ì½œë°± ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_callbacks(self) -> list:
        t_cfg = self.config.training
        log_dir = Path(self.config.paths.log_dir) / self.config.model_name
        log_dir.mkdir(parents=True, exist_ok=True)

        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor="val_loss",
                patience=t_cfg.early_stopping_patience,
                restore_best_weights=True,
                verbose=1,
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor="val_loss",
                factor=0.5,
                patience=max(t_cfg.early_stopping_patience // 2, 3),
                min_lr=1e-6,
                verbose=1,
            ),
            keras.callbacks.TensorBoard(
                log_dir=str(log_dir),
                histogram_freq=0,
                update_freq="epoch",
            ),
            keras.callbacks.ModelCheckpoint(
                filepath=str(log_dir / "best_weights"),
                monitor="val_loss",
                save_best_only=True,
                save_weights_only=True,
                verbose=0,
            ),
        ]
        return callbacks

    # â”€â”€ í•™ìŠµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: Optional[np.ndarray] = None,
        y_val: Optional[np.ndarray] = None,
    ) -> keras.callbacks.History:
        """
        ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

        Args:
            X_train: (samples, num_steps, input_size)
            y_train: (samples, num_steps, 1)
            X_val:   ê²€ì¦ ë°ì´í„° (ì—†ìœ¼ë©´ ìžë™ ë¶„í• )
            y_val:   ê²€ì¦ ë ˆì´ë¸”

        Returns:
            Keras History ê°ì²´
        """
        t_cfg = self.config.training

        if X_val is not None:
            validation_data = (X_val, y_val)
            val_split = 0.0
        else:
            validation_data = None
            val_split = t_cfg.validation_split

        print(
            f"\nðŸš€ í•™ìŠµ ì‹œìž‘ | "
            f"ìƒ˜í”Œ: {len(X_train)}, "
            f"ë°°ì¹˜: {t_cfg.batch_size}, "
            f"ì—í¬í¬: {t_cfg.epochs}"
        )

        self._history = self.model.fit(
            X_train,
            y_train,
            batch_size=t_cfg.batch_size,
            epochs=t_cfg.epochs,
            validation_split=val_split,
            validation_data=validation_data,
            shuffle=t_cfg.shuffle,
            callbacks=self._build_callbacks(),
            verbose=1,
        )

        self._save_history()
        return self._history

    # â”€â”€ ì ì§„ì  í•™ìŠµ (Gradual Train) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def gradual_train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        n_stages: int = 5,
    ) -> keras.callbacks.History:
        """
        ë°ì´í„°ë¥¼ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë ¤ê°€ë©° í•™ìŠµí•©ë‹ˆë‹¤.
        ì´ˆë°˜ì—ëŠ” ìµœê·¼ ë°ì´í„°ë¡œë§Œ í•™ìŠµí•˜ê³ , ë‹¨ê³„ë³„ë¡œ ì˜¤ëž˜ëœ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

        Args:
            X_train:  ì „ì²´ í•™ìŠµ ë°ì´í„° X
            y_train:  ì „ì²´ í•™ìŠµ ë°ì´í„° y
            n_stages: ë‹¨ê³„ ìˆ˜

        Returns:
            ë§ˆì§€ë§‰ ë‹¨ê³„ì˜ History
        """
        total = len(X_train)
        stage_size = total // n_stages
        history = None

        for stage in range(1, n_stages + 1):
            start = max(total - stage * stage_size, 0)
            X_stage = X_train[start:]
            y_stage = y_train[start:]
            print(f"\nðŸ“ˆ ì ì§„ì  í•™ìŠµ ë‹¨ê³„ {stage}/{n_stages} | ìƒ˜í”Œ: {len(X_stage)}")
            history = self.train(X_stage, y_stage)

        return history

    # â”€â”€ í‰ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def evaluate(
        self, X: np.ndarray, y: np.ndarray, label: str = "í‰ê°€"
    ) -> Dict[str, float]:
        """
        ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

        Returns:
            {"loss": ..., "rmse": ...}
        """
        results = self.model.evaluate(X, y, batch_size=1, verbose=0)
        metrics = dict(zip(self.model.metrics_names, results))
        print(f"ðŸ“Š [{label}] Loss: {metrics['loss']:.6f} | RMSE: {metrics['rmse']:.6f}")
        return metrics

    # â”€â”€ ì´ë ¥ ì €ìž¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _save_history(self):
        if self._history is None:
            return
        result_dir = Path(self.config.paths.result_dir)
        result_dir.mkdir(parents=True, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        hist_path = result_dir / f"train_history_{self.config.model_name}_{ts}.csv"
        pd.DataFrame(self._history.history).to_csv(hist_path, index=False)
        print(f"ðŸ“„ í•™ìŠµ ì´ë ¥ ì €ìž¥: {hist_path}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì˜ˆì¸¡ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Predictor:
    """
    í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì €ìž¥í•©ë‹ˆë‹¤.

    Args:
        model:  í•™ìŠµëœ keras.Model
        config: Config ë°ì´í„°í´ëž˜ìŠ¤
    """

    def __init__(self, model: keras.Model, config):
        self.model = model
        self.config = config

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        LSTM many-to-many ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ 
        ë§ˆì§€ë§‰ ìŠ¤í…ì˜ ì˜ˆì¸¡ê°’ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            X: (samples, num_steps, input_size)

        Returns:
            pred_last: (samples,) - ê° ìƒ˜í”Œì˜ ë§ˆì§€ë§‰ ìŠ¤í… ì˜ˆì¸¡ê°’
        """
        raw_pred = self.model.predict(X, batch_size=1, verbose=0)
        # raw_pred: (samples, num_steps, 1)
        pred_last = raw_pred[:, -1, 0]   # ë§ˆì§€ë§‰ ìŠ¤í…ë§Œ ì¶”ì¶œ
        return pred_last

    def predict_and_save(
        self,
        X: np.ndarray,
        y: np.ndarray,
        index_today: np.ndarray,
        date_pred: np.ndarray,
        today_list: list,
        std: Optional[np.ndarray] = None,
    ) -> pd.DataFrame:
        """
        ì˜ˆì¸¡ í›„ ë°©í–¥ì„± ì •í™•ë„Â·ì†ìµ ê³„ì‚° ë° CSV ì €ìž¥.

        Args:
            X:           í…ŒìŠ¤íŠ¸ ìž…ë ¥ ì‹œí€€ìŠ¤
            y:           í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ (samples, num_steps, 1)
            index_today: ì˜ˆì¸¡ ì‹œì ì˜ ì‹¤ì œ ì§€ìˆ˜ ê°’
            date_pred:   ì˜ˆì¸¡ ëŒ€ìƒ ë‚ ì§œ ë°°ì—´
            today_list:  ì˜ˆì¸¡ ê¸°ì¤€ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸
            std:         í‘œì¤€íŽ¸ì°¨ ë°°ì—´ (ì„ íƒ)

        Returns:
            ê²°ê³¼ DataFrame
        """
        pred_last = self.predict(X)
        target_last = y[:, -1, 0]  # (samples,)

        # ë°©í–¥ì„± ì •í™•ë„ ê³„ì‚°
        accuracy, precision, recall, f1 = calculate_metrics(target_last, pred_last)

        # ì‹¤ì œ ì§€ìˆ˜ vs ì˜ˆì¸¡ ì§€ìˆ˜
        n = min(len(index_today), len(pred_last), len(target_last))
        index_real = index_today[:n] + target_last[:n]
        index_pred_val = index_today[:n] + pred_last[:n]

        # ì†ìµ ê³„ì‚°: ë°©í–¥ì´ ë§žìœ¼ë©´ +, í‹€ë¦¬ë©´ -
        profits = []
        for i in range(n):
            diff_real = index_today[i] - index_real[i]
            diff_pred = index_today[i] - index_pred_val[i]
            sign = 1 if diff_real * diff_pred > 0 else -1
            profits.append(sign * abs(diff_real))

        result_dict = {
            "date_base": today_list[:n],
            "date_pred": date_pred[:n],
            "real_diff": target_last[:n],
            "pred_diff": pred_last[:n],
            "index_today": index_today[:n],
            "index_real": index_real,
            "index_pred": index_pred_val,
            "profit": profits,
        }
        if std is not None:
            result_dict["std"] = std[:n]

        df_result = pd.DataFrame(result_dict)

        # ê²°ê³¼ ì €ìž¥
        result_dir = Path(self.config.paths.result_dir)
        result_dir.mkdir(parents=True, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_path = result_dir / f"result_{self.config.model_name}_{ts}.csv"

        df_result.to_csv(result_path, index=False)

        summary_path = str(result_path).replace(".csv", "_summary.txt")
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write(f"ì˜ˆì¸¡ ê¸°ê°„: {self.config.prediction.predict_term}ì¼\n")
            f.write(f"ëª¨ë¸: {self.config.model_name}\n")
            f.write(f"ë°©í–¥ ì •í™•ë„: {accuracy:.4f}\n")
            f.write(f"Precision: {precision:.4f}\n")
            f.write(f"Recall: {recall:.4f}\n")
            f.write(f"F1 Score: {f1:.4f}\n")
            f.write(f"ì´ ëˆ„ì  ì†ìµ: {sum(profits):.4f}\n")

        print(f"\nðŸ“Š ì˜ˆì¸¡ ê²°ê³¼")
        print(f"   ë°©í–¥ ì •í™•ë„ : {accuracy:.4f}")
        print(f"   Precision   : {precision:.4f}")
        print(f"   Recall      : {recall:.4f}")
        print(f"   F1 Score    : {f1:.4f}")
        print(f"   ëˆ„ì  ì†ìµ   : {sum(profits):.4f}")
        print(f"ðŸ“„ ê²°ê³¼ ì €ìž¥   : {result_path}")

        return df_result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def calculate_metrics(
    labels: np.ndarray, predictions: np.ndarray
) -> Tuple[float, float, float, float]:
    """
    ë°©í–¥ì„± ê¸°ë°˜ ì •í™•ë„, Precision, Recall, F1ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        labels:      ì‹¤ì œ ê°’ ë°°ì—´
        predictions: ì˜ˆì¸¡ ê°’ ë°°ì—´

    Returns:
        (accuracy, precision, recall, f1_score)
    """
    tp = fp = tn = fn = 0

    for label, pred in zip(labels, predictions):
        if pred > 0:
            if label > 0:
                tp += 1
            else:
                fp += 1
        else:
            if label < 0:
                tn += 1
            else:
                fn += 1

    total = tp + fp + tn + fn
    accuracy = (tp + tn) / total if total > 0 else 0.0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = (
        2 / (1 / precision + 1 / recall)
        if (precision > 0 and recall > 0)
        else 0.0
    )

    return accuracy, precision, recall, f1
